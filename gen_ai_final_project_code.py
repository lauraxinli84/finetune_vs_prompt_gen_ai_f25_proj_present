# -*- coding: utf-8 -*-
"""gen_ai_final_project_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TnDh7Dh-3KNf4tiXUj-z8fS63fM6VIw1
"""

!pip uninstall -y bitsandbytes

"""# Prompt Engineering vs. Fine-Tuning: Cross-Domain Sentiment Classification

**Project Overview:** Comparing baseline (zero-shot), prompt engineering, and fine-tuning (LoRA) on 5-class sentiment classification for Yelp and Amazon reviews.

---

## Environment Setup

Install required packages and check GPU availability.
"""

# Install required packages
!pip install transformers==4.44.0 datasets==2.20.0 accelerate==0.33.0 peft==0.12.0 scikit-learn

# Check GPU availability
import torch
print(f"GPU Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU Name: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

# Mount and login
from google.colab import drive
drive.mount('/content/drive')

from huggingface_hub import login
login()  # Enter token

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import Dataset, DatasetDict
import pandas as pd
import numpy as np
from peft import LoraConfig, get_peft_model, TaskType
from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score, classification_report
import re, json, pickle
from datetime import datetime
import gc

np.random.seed(42)

"""## Load Datasets

Loading Yelp Review Full and Amazon Reviews Full (both 5-class sentiment).
"""

from datasets import load_dataset, Dataset, DatasetDict
import pandas as pd
import numpy as np
from google.colab import drive
import csv

# Mount Google Drive
print("Mounting Google Drive...")
drive.mount('/content/drive')

# Set random seed for reproducibility
np.random.seed(42)

# Load Yelp dataset (5-class sentiment: 1-5 stars, labels 0-4)
print("\nLoading Yelp Review Full dataset from Hugging Face...")
print("Source: https://huggingface.co/datasets/Yelp/yelp_review_full")
yelp_full = load_dataset("Yelp/yelp_review_full")

# Load Amazon dataset from Kaggle CSV
print("\nLoading Amazon dataset from Kaggle CSV...")
print("Source: Amazon Reviews fine-grained 5 classes (Kaggle)")

# Update this path to match where you saved the files in your Drive
amazon_train_path = "/content/drive/MyDrive/train.csv"
amazon_test_path = "/content/drive/MyDrive/test.csv"

# Custom function to load the Amazon CSV with proper parsing
def load_amazon_csv(filepath):
    data = []
    with open(filepath, 'r', encoding='utf-8') as f:
        csv_reader = csv.reader(f, quotechar='"', doublequote=True, strict=False)
        for i, row in enumerate(csv_reader):
            # Skip header row if present
            if i == 0 and (row[0] == 'class_index' or not row[0].isdigit()):
                continue

            if len(row) >= 3:
                # First column is label, second is title, rest is review text
                try:
                    label = int(row[0])
                    review_title = row[1]
                    review_text = row[2] if len(row) == 3 else ','.join(row[2:])
                    data.append({
                        'label': label - 1,  # Convert from 1-5 to 0-4
                        'review_title': review_title,
                        'review_text': review_text,
                        'text': review_text  # Use just review_text
                        # Or combine: 'text': review_title + '. ' + review_text
                    })
                except ValueError:
                    # Skip rows that can't be parsed
                    print(f"Skipping row {i}: {row[0]}")
                    continue
    return pd.DataFrame(data)

print("Loading train.csv...")
amazon_train_df = load_amazon_csv(amazon_train_path)

print("Loading test.csv...")
amazon_test_df = load_amazon_csv(amazon_test_path)

# Keep only needed columns
amazon_train_df = amazon_train_df[['text', 'label']]
amazon_test_df = amazon_test_df[['text', 'label']]

# Convert to HuggingFace Dataset format
amazon_full = DatasetDict({
    'train': Dataset.from_pandas(amazon_train_df, preserve_index=False),
    'test': Dataset.from_pandas(amazon_test_df, preserve_index=False)
})

print("\n=== Dataset Information ===")
print(f"Yelp - Train: {len(yelp_full['train']):,} samples")
print(f"Yelp - Test: {len(yelp_full['test']):,} samples")
print(f"\nAmazon - Train: {len(amazon_full['train']):,} samples")
print(f"Amazon - Test: {len(amazon_full['test']):,} samples")

# Verify label distributions
print("\n=== Label Distribution Check ===")
print("Yelp label range:", min(yelp_full['train']['label']), "-", max(yelp_full['train']['label']))
print("Amazon label range:", min(amazon_full['train']['label']), "-", max(amazon_full['train']['label']))

# Verify class balance
from collections import Counter
print("\nYelp train label distribution:", Counter(yelp_full['train']['label']))
print("Amazon train label distribution:", Counter(amazon_full['train']['label']))

# Check sample data
print("\n=== Sample Data Structure ===")
print("\nYelp sample:", yelp_full['train'][0])
print("\nAmazon sample:", amazon_full['train'][0])

# Subsample Amazon dataset to match scale with Yelp
print("\n=== Subsampling Amazon Dataset ===")
print("Original Amazon size - Train: 3,000,000, Test: 650,000")

# We'll subsample to a more manageable size that still gives us plenty of data
# Target: ~200k train, ~50k test (similar scale to Yelp)
amazon_train_subset = amazon_full['train'].shuffle(seed=42).select(range(200000))
amazon_test_subset = amazon_full['test'].shuffle(seed=42).select(range(50000))

# Update the amazon_full dataset
amazon_full = DatasetDict({
    'train': amazon_train_subset,
    'test': amazon_test_subset
})

print(f"\nSubsampled Amazon - Train: {len(amazon_full['train']):,} samples")
print(f"Subsampled Amazon - Test: {len(amazon_full['test']):,} samples")

# Verify class distribution is still balanced
from collections import Counter
print("\nAmazon subsampled label distribution:", Counter(amazon_full['train']['label']))

"""## Create Train/Val/Test Splits

Creating stratified splits: **5,000 train / 1,000 val / 3,000 test** per dataset.

**Why these sizes?**
- **Train (5k)**: 1,000 samples per class - sufficient for LoRA fine-tuning to show its potential
- **Val (1k)**: 200 samples per class - reliable for hyperparameter selection (LoRA rank)
- **Test (3k)**: 600 samples per class - robust evaluation metrics
- **Total**: 9,000 samples per dataset (still very manageable)
"""

def create_splits(dataset, train_size=5000, val_size=1000, test_size=3000, label_field='label'):
    """
    Create stratified train/val/test splits - OPTIMIZED VERSION.
    """
    print(f"Creating splits from dataset with {len(dataset['train']):,} train samples...")

    # Convert to pandas for faster filtering
    train_df = dataset['train'].to_pandas()

    # Detect label range
    min_label = train_df[label_field].min()
    max_label = train_df[label_field].max()
    num_classes = max_label - min_label + 1

    print(f"Detected {num_classes} classes: labels from {min_label} to {max_label}")

    samples_per_class_train = train_size // num_classes
    samples_per_class_val = val_size // num_classes

    train_indices = []
    val_indices = []

    # Sample from each class
    for label in range(min_label, max_label + 1):
        # Get indices for this class
        class_mask = train_df[label_field] == label
        class_indices = train_df[class_mask].index.tolist()

        # Shuffle and sample
        np.random.shuffle(class_indices)
        train_indices.extend(class_indices[:samples_per_class_train])
        val_indices.extend(class_indices[samples_per_class_train:samples_per_class_train + samples_per_class_val])

    # Create splits using select
    train_split = dataset['train'].select(train_indices)
    val_split = dataset['train'].select(val_indices)
    test_split = dataset['test'].shuffle(seed=42).select(range(min(test_size, len(dataset['test']))))

    return {
        'train': train_split,
        'val': val_split,
        'test': test_split
    }

# Create splits for both datasets (5k train / 1k val / 3k test)
print("Creating Yelp splits (5,000 train / 1,000 val / 3,000 test)...")
yelp_splits = create_splits(yelp_full, train_size=5000, val_size=1000, test_size=3000)

print("\nCreating Amazon splits (5,000 train / 1,000 val / 3,000 test)...")
amazon_splits = create_splits(amazon_full, train_size=5000, val_size=1000, test_size=3000)

print("\n=== Split Sizes ===")
print(f"Yelp - Train: {len(yelp_splits['train']):,}, Val: {len(yelp_splits['val']):,}, Test: {len(yelp_splits['test']):,}")
print(f"Amazon - Train: {len(amazon_splits['train']):,}, Val: {len(amazon_splits['val']):,}, Test: {len(amazon_splits['test']):,}")
print(f"\nTotal samples per dataset: {len(yelp_splits['train']) + len(yelp_splits['val']) + len(yelp_splits['test']):,}")

# Verify class distribution in splits
def check_class_distribution(split, name, label_field='label'):
    labels = [example[label_field] for example in split]
    unique, counts = np.unique(labels, return_counts=True)
    print(f"\n{name} class distribution:")
    for label, count in zip(unique, counts):
        print(f"  Class {label}: {count} samples ({count/len(labels)*100:.1f}%)")

print("=== Yelp Class Distributions ===")
check_class_distribution(yelp_splits['train'], "Yelp Train")
check_class_distribution(yelp_splits['val'], "Yelp Val")
check_class_distribution(yelp_splits['test'], "Yelp Test")

print("\n=== Amazon Class Distributions ===")
amazon_label_field = 'stars' if 'stars' in amazon_full['train'].column_names else 'label'
check_class_distribution(amazon_splits['train'], "Amazon Train", amazon_label_field)
check_class_distribution(amazon_splits['val'], "Amazon Val", amazon_label_field)
check_class_distribution(amazon_splits['test'], "Amazon Test", amazon_label_field)

"""## Load Gemma Model

Loading Gemma-2-2B-it
"""

# Model configuration
model_id = "google/gemma-2-2b-it"
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map="auto", trust_remote_code=True)
model.config.use_cache = False
print(f"Model: {torch.cuda.memory_allocated()/1e9:.2f} GB")

"""## Baseline (Zero-Shot) Evaluation

Simple zero-shot prompting as baseline.
"""

def create_zero_shot_prompt(text):
    """
    Create a zero-shot prompt for sentiment classification.

    Returns:
        Formatted prompt string
    """
    prompt = f"""Classify the sentiment of the following review on a scale of 1 to 5:
1 = Very Negative
2 = Negative
3 = Neutral
4 = Positive
5 = Very Positive

Review: {text}

Sentiment (1-5):"""
    return prompt

# Test on a sample
sample_text = yelp_splits['test'][0]['text']
sample_label = yelp_splits['test'][0]['label']

prompt = create_zero_shot_prompt(sample_text)
print("=== Sample Zero-Shot Prompt ===")
print(prompt)
print(f"\nTrue label: {sample_label + 1}")

"""### Generate Model Predictions

Function to generate predictions from the model and extract sentiment ratings.
"""

import re
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score, classification_report
import torch

def generate_prediction(prompt, max_new_tokens=10):
    """
    Generate model prediction for a given prompt.
    """
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.1,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    # Decode only the generated tokens
    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    return generated_text

def extract_rating(response_text):
    """
    Extract rating (1-5) from model response.
    """
    # Try to find a number between 1-5
    matches = re.findall(r'\b[1-5]\b', response_text)
    if matches:
        return int(matches[0])
    return None

def evaluate_zero_shot(dataset_split, dataset_name, num_samples=None):
    """
    Evaluate zero-shot performance on a dataset.
    """
    if num_samples is None:
        num_samples = len(dataset_split)

    predictions = []
    true_labels = []
    failed_parses = 0

    print(f"\nEvaluating {dataset_name} zero-shot on {num_samples} samples...")

    for i in tqdm(range(num_samples)):
        text = dataset_split[i]['text']
        true_label = dataset_split[i]['label']

        # Create prompt
        prompt = create_zero_shot_prompt(text)

        # Generate response
        response = generate_prediction(prompt, max_new_tokens=10)

        # Extract predicted rating
        pred = extract_rating(response)

        if pred is not None:
            # Convert back to 0-4 scale (model outputs 1-5)
            predictions.append(pred - 1)
            true_labels.append(true_label)
        else:
            failed_parses += 1
            # Default to middle class if parsing fails
            predictions.append(2)
            true_labels.append(true_label)

    # Calculate metrics
    accuracy = accuracy_score(true_labels, predictions)
    f1_macro = f1_score(true_labels, predictions, average='macro')
    f1_weighted = f1_score(true_labels, predictions, average='weighted')

    print(f"\n{'='*50}")
    print(f"{dataset_name} Zero-Shot Results")
    print(f"{'='*50}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 (Macro): {f1_macro:.4f}")
    print(f"F1 (Weighted): {f1_weighted:.4f}")
    print(f"Failed parses: {failed_parses}/{num_samples}")
    print(f"\nClassification Report:")
    print(classification_report(true_labels, predictions,
                                target_names=['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4']))

    return {
        'accuracy': accuracy,
        'f1_macro': f1_macro,
        'f1_weighted': f1_weighted,
        'predictions': predictions,
        'true_labels': true_labels,
        'failed_parses': failed_parses
    }

"""### Full Yelp Test Evaluation

Evaluate zero-shot performance on the complete Yelp test set (3,000 samples).
"""

# Full evaluation on Yelp test set (3000 samples)
print("\n" + "="*70)
print("FULL YELP TEST SET EVALUATION")
print("="*70)
yelp_zero_shot_results = evaluate_zero_shot(yelp_splits['test'], "Yelp Test")

import json
import pickle
from datetime import datetime

# Create timestamp
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# Save results as JSON (metrics only)
yelp_results_summary = {
    'dataset': 'Yelp',
    'method': 'zero_shot',
    'timestamp': timestamp,
    'num_samples': 3000,
    'accuracy': yelp_zero_shot_results['accuracy'],
    'f1_macro': yelp_zero_shot_results['f1_macro'],
    'f1_weighted': yelp_zero_shot_results['f1_weighted'],
    'failed_parses': yelp_zero_shot_results['failed_parses']
}

with open(f'/content/drive/MyDrive/yelp_zero_shot_results_{timestamp}.json', 'w') as f:
    json.dump(yelp_results_summary, f, indent=2)

# Save full results including predictions (pickle)
with open(f'/content/drive/MyDrive/yelp_zero_shot_full_{timestamp}.pkl', 'wb') as f:
    pickle.dump(yelp_zero_shot_results, f)

print("Results saved to Google Drive:")
print(f"   - Summary: yelp_zero_shot_results_{timestamp}.json")
print(f"   - Full data: yelp_zero_shot_full_{timestamp}.pkl")

"""### Full Amazon Test Evaluation

Evaluate zero-shot performance on the complete Amazon test set (3,000 samples).
"""

# Full evaluation on Amazon test set (3000 samples)
print("\n" + "="*70)
print("FULL AMAZON TEST SET EVALUATION")
print("="*70)
amazon_zero_shot_results = evaluate_zero_shot(amazon_splits['test'], "Amazon Test")

import json
import pickle
from datetime import datetime

# Create timestamp
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# Save results as JSON (metrics only)
amazon_results_summary = {
    'dataset': 'Amazon',
    'method': 'zero_shot',
    'timestamp': timestamp,
    'num_samples': 3000,
    'accuracy': amazon_zero_shot_results['accuracy'],
    'f1_macro': amazon_zero_shot_results['f1_macro'],
    'f1_weighted': amazon_zero_shot_results['f1_weighted'],
    'failed_parses': amazon_zero_shot_results['failed_parses']
}

with open(f'/content/drive/MyDrive/amazon_zero_shot_results_{timestamp}.json', 'w') as f:
    json.dump(amazon_results_summary, f, indent=2)

# Save full results including predictions (pickle)
with open(f'/content/drive/MyDrive/amazon_zero_shot_full_{timestamp}.pkl', 'wb') as f:
    pickle.dump(amazon_zero_shot_results, f)

print("Amazon results saved to Google Drive:")
print(f"   - Summary: amazon_zero_shot_results_{timestamp}.json")
print(f"   - Full data: amazon_zero_shot_full_{timestamp}.pkl")

# Quick comparison
print("\n" + "="*70)
print("ZERO-SHOT BASELINE COMPARISON")
print("="*70)
print(f"Yelp    - Accuracy: {yelp_zero_shot_results['accuracy']:.4f}, F1 (Macro): {yelp_zero_shot_results['f1_macro']:.4f}")
print(f"Amazon  - Accuracy: {amazon_zero_shot_results['accuracy']:.4f}, F1 (Macro): {amazon_zero_shot_results['f1_macro']:.4f}")

"""## Few-Shot Prompting (5-shot)

Add 5 examples, one from each class, to the prompt to help the model learn the task through demonstration.

### Select Representative Examples

Choose balanced examples from the training set (one per class).

## Run on Yelp Dataset
"""

import numpy as np

# Extract rating from response
def extract_rating_improved(response_text):
    """
    Improved extraction that handles various formats.
    """
    cleaned = response_text.replace('**', '').replace('*', '').strip()

    # Look for number at start
    if len(cleaned) > 0 and cleaned[0] in '12345':
        return int(cleaned[0])

    # Pattern matching
    patterns = [
        r'[Ss]entiment[:\s]+([1-5])',
        r'[Rr]ating[:\s]+([1-5])',
        r'^([1-5])\s*$',
        r'^([1-5])\s*\n',
    ]

    for pattern in patterns:
        match = re.search(pattern, cleaned)
        if match:
            return int(match.group(1))

    # Find any number 1-5
    matches = re.findall(r'\b([1-5])\b', cleaned)
    if matches:
        return int(matches[-1])

    return None

def select_few_shot_examples(train_split, n_shots=5, label_field='label'):
    """
    Select one example from each class for balanced representation.
    """
    examples = []
    num_classes = 5
    train_data = list(train_split)

    for label in range(num_classes):
        class_samples = [s for s in train_data if s[label_field] == label]
        if len(class_samples) > 0:
            idx = np.random.randint(0, len(class_samples))
            examples.append(class_samples[idx])

    return examples

def create_few_shot_prompt(text, examples, label_field='label'):
    prompt = """Classify the sentiment of reviews on a scale of 1 to 5:
1 = Very Negative
2 = Negative
3 = Neutral
4 = Positive
5 = Very Positive

Here are some examples:

"""

    for ex in examples:
        example_text = ex['text']
        example_label = ex[label_field] + 1
        prompt += f"Review: {example_text}\nSentiment: {example_label}\n\n"

    # Change this - remove the parenthetical instruction
    prompt += f"Review: {text}\nSentiment:"

    return prompt

# Select 5-shot examples
np.random.seed(42)
yelp_5shot_examples = select_few_shot_examples(yelp_splits['train'], n_shots=5)

print("Selected 5-shot examples - classes:", [ex['label'] for ex in yelp_5shot_examples])

# Generating predictions
def generate_prediction(prompt, max_new_tokens=15):
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True,
                      max_length=2048).to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.1,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )

    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:],
                                     skip_special_tokens=True)
    return generated_text

# Evaluate on Yelp
def evaluate_few_shot(dataset_split, dataset_name, examples, num_samples=None, label_field='label'):
    if num_samples is None:
        num_samples = len(dataset_split)

    predictions = []
    true_labels = []
    failed_parses = 0

    print(f"\nEvaluating {dataset_name} few-shot ({len(examples)} examples) on {num_samples} samples...")

    for i in tqdm(range(num_samples)):
        text = dataset_split[i]['text']
        true_label = dataset_split[i][label_field]

        prompt = create_few_shot_prompt(text, examples, label_field)
        response = generate_prediction(prompt, max_new_tokens=5)
        pred = extract_rating_improved(response)

        if pred is not None:
            predictions.append(pred - 1)
            true_labels.append(true_label)
        else:
            failed_parses += 1
            predictions.append(2)
            true_labels.append(true_label)

    accuracy = accuracy_score(true_labels, predictions)
    f1_macro = f1_score(true_labels, predictions, average='macro')
    f1_weighted = f1_score(true_labels, predictions, average='weighted')

    print(f"\n{'='*50}")
    print(f"{dataset_name} 5-Shot Results")
    print(f"{'='*50}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 (Macro): {f1_macro:.4f}")
    print(f"F1 (Weighted): {f1_weighted:.4f}")
    print(f"Failed parses: {failed_parses}/{num_samples}")

    return {
        'accuracy': accuracy,
        'f1_macro': f1_macro,
        'f1_weighted': f1_weighted,
        'predictions': predictions,
        'true_labels': true_labels,
        'failed_parses': failed_parses
    }

# Run evaluation
yelp_5shot_results = evaluate_few_shot(yelp_splits['test'], "Yelp Test", yelp_5shot_examples)

# Save results
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
with open(f'/content/drive/MyDrive/yelp_5shot_improved_{timestamp}.json', 'w') as f:
    json.dump({
        'dataset': 'Yelp',
        'method': '5shot_improved',
        'timestamp': timestamp,
        'num_samples': 3000,
        'accuracy': yelp_5shot_results['accuracy'],
        'f1_macro': yelp_5shot_results['f1_macro'],
        'f1_weighted': yelp_5shot_results['f1_weighted'],
        'failed_parses': yelp_5shot_results['failed_parses']
    }, f, indent=2)

"""### Few-Shot Prompting on Amazon

Apply the same 5-shot approach to Amazon to test cross-domain consistency.



"""

# Select 5-shot examples for Amazon
np.random.seed(42)
amazon_5shot_examples = select_few_shot_examples(amazon_splits['train'], n_shots=5)

print("Selected 5-shot examples - classes:", [ex['label'] for ex in amazon_5shot_examples])

# Run evaluation
amazon_5shot_results = evaluate_few_shot(amazon_splits['test'], "Amazon Test", amazon_5shot_examples)

# Save results
with open(f'/content/drive/MyDrive/amazon_5shot_improved_{timestamp}.json', 'w') as f:
    json.dump({
        'dataset': 'Amazon',
        'method': '5shot_improved',
        'timestamp': timestamp,
        'num_samples': 1000,
        'accuracy': amazon_5shot_results['accuracy'],
        'f1_macro': amazon_5shot_results['f1_macro'],
        'f1_weighted': amazon_5shot_results['f1_weighted'],
        'failed_parses': amazon_5shot_results['failed_parses']
    }, f, indent=2)

"""## LoRA Fine-Tuning on Yelp

Fine-tune the model using LoRA (Low-Rank Adaptation) with different rank values to compare parameter efficiency.

**Why LoRA?**
- LoRA adapts only a small subset of parameters
- Efficient training on consumer GPUs

**I'll test:**
- r=8 (standard, efficient)

### Prepare Data for Fine-Tuning

Format the training data for the model with proper instruction templates.
"""

# Format training data
def format_instruction(example):
    text = example['text']
    label = example['label'] + 1  # Convert 0-4 to 1-5

    prompt = f"""Classify the sentiment of the following review on a scale of 1 to 5:
1 = Very Negative
2 = Negative
3 = Neutral
4 = Positive
5 = Very Positive

Review: {text}

Sentiment (1-5): {label}"""

    return {"formatted_text": prompt}

print("Formatting datasets...")
train_dataset = yelp_splits['train'].map(format_instruction)
val_dataset = yelp_splits['val'].map(format_instruction)

print(f"Formatted {len(train_dataset)} train, {len(val_dataset)} val samples")

# Tokenize with SHORT sequences to save memory
def tokenize_function(examples):
    tokenized = tokenizer(
        examples['formatted_text'],
        truncation=True,
        max_length=128,  # Very short to save memory
        padding='max_length',
    )
    tokenized['labels'] = tokenized['input_ids'].copy()
    return tokenized

print("Tokenizing...")
tokenized_train = train_dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=train_dataset.column_names,
    desc="Tokenizing train"
)

tokenized_val = val_dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=val_dataset.column_names,
    desc="Tokenizing val"
)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

"""### Configure LoRA for Fine-Tuning

Set up LoRA configuration for efficient parameter adaptation.
"""

import gc
torch.cuda.empty_cache()
gc.collect()

# Reload base model
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

# Enable gradient checkpointing BEFORE adding LoRA
model.gradient_checkpointing_enable()
model.enable_input_require_grads()
model.config.use_cache = False

# Now create LoRA model
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

model_lora = get_peft_model(model, lora_config)

# Verify trainability
trainable = sum(p.numel() for p in model_lora.parameters() if p.requires_grad)
total = sum(p.numel() for p in model_lora.parameters())
print(f"\nTrainable: {trainable:,} ({100*trainable/total:.2f}%)")

# Check that some parameters actually require grad
for name, param in model_lora.named_parameters():
    if param.requires_grad:
        print(f"Found trainable param: {name}")
        break

# memory-optimized training settings
training_args = TrainingArguments(
    output_dir=f"/content/drive/MyDrive/yelp_lora_r8",
    num_train_epochs=2,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    warmup_steps=50,
    logging_steps=100,
    eval_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=1,
    load_best_model_at_end=True,
    fp16=True,
    optim="adamw_torch",
    report_to="none",
    remove_unused_columns=False,
)

trainer = Trainer(
    model=model_lora,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    data_collator=data_collator,
)

print("\n Starting training...")
train_result = trainer.train()

print(f"\n Training complete")
print(f"Final train loss: {train_result.training_loss:.4f}")

# Save model
model_lora.save_pretrained("/content/drive/MyDrive/yelp_lora_r8_final")
tokenizer.save_pretrained("/content/drive/MyDrive/yelp_lora_r8_final")
print("Model saved to Drive")

"""### Evaluate Fine-Tuned Model on Yelp Test Set

Test the LoRA r=8 model on Yelp test data.
"""

# Improved extraction function (same as before)
import re

def extract_rating_improved(response_text):
    """
    Improved extraction that handles various formats.
    """
    cleaned = response_text.replace('**', '').replace('*', '').strip()

    # Strategy 1: Look for patterns
    patterns = [
        r'[Ss]entiment[:\s]+([1-5])',
        r'[Rr]ating[:\s]+([1-5])',
        r'^([1-5])\s*$',
        r'^([1-5])\s*\n',
    ]

    for pattern in patterns:
        match = re.search(pattern, cleaned)
        if match:
            return int(match.group(1))

    # Strategy 2: Find ANY number 1-5
    matches = re.findall(r'\b([1-5])\b', cleaned)
    if matches:
        return int(matches[-1])  # Take last occurrence

    return None

def evaluate_finetuned(model, tokenizer, dataset_split, dataset_name, num_samples=None):
    """
    Evaluate fine-tuned model performance.
    """
    if num_samples is None:
        num_samples = len(dataset_split)

    predictions = []
    true_labels = []
    failed_parses = 0

    print(f"\nEvaluating {dataset_name} on {num_samples} samples...")

    model.eval()

    for i in tqdm(range(num_samples)):
        text = dataset_split[i]['text']
        true_label = dataset_split[i]['label']

        # Create prompt (same format as training)
        prompt = f"""Classify the sentiment of the following review on a scale of 1 to 5:
1 = Very Negative
2 = Negative
3 = Neutral
4 = Positive
5 = Very Positive

Review: {text}

Sentiment (1-5):"""

        # Generate prediction
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=10,
                temperature=0.1,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )

        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)

        # Extract rating
        pred = extract_rating_improved(response)

        if pred is not None:
            predictions.append(pred - 1)  # Convert to 0-4
            true_labels.append(true_label)
        else:
            failed_parses += 1
            predictions.append(2)
            true_labels.append(true_label)

    # Calculate metrics
    accuracy = accuracy_score(true_labels, predictions)
    f1_macro = f1_score(true_labels, predictions, average='macro')
    f1_weighted = f1_score(true_labels, predictions, average='weighted')

    print(f"\n{'='*50}")
    print(f"{dataset_name} Fine-Tuned (LoRA r=8) Results")
    print(f"{'='*50}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 (Macro): {f1_macro:.4f}")
    print(f"F1 (Weighted): {f1_weighted:.4f}")
    print(f"Failed parses: {failed_parses}/{num_samples}")
    print(f"\nClassification Report:")
    print(classification_report(true_labels, predictions,
                                target_names=['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4'],
                                zero_division=0))

    return {
        'accuracy': accuracy,
        'f1_macro': f1_macro,
        'f1_weighted': f1_weighted,
        'predictions': predictions,
        'true_labels': true_labels,
        'failed_parses': failed_parses
    }

# Evaluate on Yelp test set
print("\n" + "="*70)
print("EVALUATION: LoRA r=8 on Yelp Test")
print("="*70)
yelp_finetuned_results = evaluate_finetuned(model_lora, tokenizer, yelp_splits['test'], "Yelp Test")

# Save results
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

with open(f'/content/drive/MyDrive/yelp_finetuned_r8_results_{timestamp}.json', 'w') as f:
    json.dump({
        'dataset': 'Yelp',
        'method': 'lora_finetuned',
        'lora_rank': 8,
        'timestamp': timestamp,
        'num_samples': 3000,
        'accuracy': yelp_finetuned_results['accuracy'],
        'f1_macro': yelp_finetuned_results['f1_macro'],
        'f1_weighted': yelp_finetuned_results['f1_weighted'],
        'failed_parses': yelp_finetuned_results['failed_parses'],
        'train_loss': train_result.training_loss
    }, f, indent=2)

with open(f'/content/drive/MyDrive/yelp_finetuned_r8_full_{timestamp}.pkl', 'wb') as f:
    pickle.dump(yelp_finetuned_results, f)

print("Results saved")

"""### Evaluate on Amazon Test Set (Transfer Learning)

Test whether the Yelp-trained model generalizes to Amazon reviews.
"""

# Evaluate on Amazon test set (cross-domain transfer)
print("\n" + "="*70)
print("EVALUATION: LoRA r=8 on Amazon Test (Transfer Learning)")
print("="*70)
amazon_finetuned_results = evaluate_finetuned(model_lora, tokenizer, amazon_splits['test'], "Amazon Test (Transfer)")

# Save Amazon results
with open(f'/content/drive/MyDrive/amazon_finetuned_transfer_{timestamp}.json', 'w') as f:
    json.dump({
        'dataset': 'Amazon',
        'method': 'lora_finetuned_transfer',
        'lora_rank': 8,
        'trained_on': 'Yelp',
        'timestamp': timestamp,
        'num_samples': 3000,
        'accuracy': amazon_finetuned_results['accuracy'],
        'f1_macro': amazon_finetuned_results['f1_macro'],
        'f1_weighted': amazon_finetuned_results['f1_weighted'],
        'failed_parses': amazon_finetuned_results['failed_parses']
    }, f, indent=2)

print(" Amazon transfer results saved")

"""### Final Results Comparison

Complete comparison of all approaches across both datasets.
"""

# Load all saved results from Google Drive
import json
import pickle
from glob import glob

# Find the most recent result files
def load_latest_json(pattern):
    files = glob(pattern)
    if not files:
        raise FileNotFoundError(f"No files found matching: {pattern}")
    latest = max(files, key=lambda x: x.split('_')[-1])
    with open(latest, 'r') as f:
        return json.load(f)

# Load Yelp results
yelp_zero_shot_data = load_latest_json('/content/drive/MyDrive/yelp_zero_shot_results_*.json')
yelp_5shot_data = load_latest_json('/content/drive/MyDrive/yelp_5shot_improved_*.json')
yelp_finetuned_data = load_latest_json('/content/drive/MyDrive/yelp_finetuned_r8_results_*.json')

# Load Amazon results
amazon_zero_shot_data = load_latest_json('/content/drive/MyDrive/amazon_zero_shot_results_*.json')
amazon_5shot_data = load_latest_json('/content/drive/MyDrive/amazon_5shot_improved_*.json')
amazon_finetuned_data = load_latest_json('/content/drive/MyDrive/amazon_finetuned_transfer_*.json')

# Display final comparison
print("="*70)
print("FINAL RESULTS: ALL METHODS")
print("="*70)

print("\n YELP RESULTS:")
print(f"{'Method':<25} {'Accuracy':<12} {'F1 (Macro)':<12} {'Failed Parses'}")
print("-" * 65)
print(f"{'Zero-shot':<25} {yelp_zero_shot_data['accuracy']:<12.4f} {yelp_zero_shot_data['f1_macro']:<12.4f} {yelp_zero_shot_data['failed_parses']}")
print(f"{'5-shot':<25} {yelp_5shot_data['accuracy']:<12.4f} {yelp_5shot_data['f1_macro']:<12.4f} {yelp_5shot_data['failed_parses']}")
print(f"{'Fine-tuned (LoRA r=8)':<25} {yelp_finetuned_data['accuracy']:<12.4f} {yelp_finetuned_data['f1_macro']:<12.4f} {yelp_finetuned_data['failed_parses']}")

print("\n AMAZON RESULTS:")
print(f"{'Method':<25} {'Accuracy':<12} {'F1 (Macro)':<12} {'Failed Parses'}")
print("-" * 65)
print(f"{'Zero-shot':<25} {amazon_zero_shot_data['accuracy']:<12.4f} {amazon_zero_shot_data['f1_macro']:<12.4f} {amazon_zero_shot_data['failed_parses']}")
print(f"{'5-shot':<25} {amazon_5shot_data['accuracy']:<12.4f} {amazon_5shot_data['f1_macro']:<12.4f} {amazon_5shot_data['failed_parses']}")
print(f"{'Fine-tuned (transfer)':<25} {amazon_finetuned_data['accuracy']:<12.4f} {amazon_finetuned_data['f1_macro']:<12.4f} {amazon_finetuned_data['failed_parses']}")

print("\n" + "="*70)